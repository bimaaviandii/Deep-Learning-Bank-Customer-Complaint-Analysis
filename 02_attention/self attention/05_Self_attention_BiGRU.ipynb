{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LQy698OIo25G"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from sklearn.model_selection import train_test_split\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "\n",
        "\n",
        "import warnings;\n",
        "warnings.filterwarnings('ignore');"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RGu55wWuo3fh"
      },
      "source": [
        "### Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "df = pd.read_csv(\"result/01_date_preprocessing_result.csv\",sep=\",\")\n",
        "df.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Splitting Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# The maximum number of words to be used. (most frequent)\n",
        "MAX_NB_WORDS = 50000\n",
        "# Max number of words in each complaint.\n",
        "MAX_SEQUENCE_LENGTH = 250\n",
        "# This is fixed.\n",
        "EMBEDDING_DIM = 100\n",
        "\n",
        "tokenizer = Tokenizer(num_words=MAX_NB_WORDS, \n",
        "                      filters='!\"#$%&()*+,-./:;<=>?@[\\]^_`{|}~', \n",
        "                      lower=True)\n",
        "tokenizer.fit_on_texts(df['narrative'].values)\n",
        "word_index = tokenizer.word_index\n",
        "print('Found %s unique tokens.' % len(word_index))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X = tokenizer.texts_to_sequences(df['narrative'].values)\n",
        "X = pad_sequences(X, maxlen=MAX_SEQUENCE_LENGTH)\n",
        "print('Shape of data tensor:', X.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "y = np.argmax(df[['credit_card', 'credit_reporting', 'debt_collection', 'mortgages_and_loans', 'retail_banking']].values, axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(X,y,test_size=0.2 , random_state=22)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(X_train.shape,y_train.shape)\n",
        "print(X_test.shape,y_test.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Modeling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential, Model\n",
        "from tensorflow.keras.callbacks import EarlyStopping\n",
        "from tensorflow.keras.layers import Input, Dropout, Attention, MultiHeadAttention, Flatten,  Embedding, GRU, Dense, GlobalMaxPooling1D, LayerNormalization, GlobalAveragePooling1D, Add, Bidirectional"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Self-Attention Layer definition\n",
        "class SelfAttention(tf.keras.layers.Layer):\n",
        "    def __init__(self, num_units):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.num_units = num_units\n",
        "        self.query_dense = Dense(num_units)\n",
        "        self.key_dense = Dense(num_units)\n",
        "        self.value_dense = Dense(num_units)\n",
        "        self.output_dense = Dense(num_units)\n",
        "\n",
        "    def call(self, inputs):\n",
        "        query = self.query_dense(inputs)\n",
        "        key = self.key_dense(inputs)\n",
        "        value = self.value_dense(inputs)\n",
        "        attention_scores = tf.matmul(query, key, transpose_b=True)\n",
        "        attention_scores = attention_scores / tf.sqrt(tf.cast(self.num_units, tf.float32))\n",
        "        attention_weights = tf.nn.softmax(attention_scores, axis=-1)\n",
        "        attention_output = tf.matmul(attention_weights, value)\n",
        "        return self.output_dense(attention_output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#fine tuning\n",
        "num_heads = 4\n",
        "num_units = 64\n",
        "dropout_rate = 0.4\n",
        "\n",
        "num_units_1 = 32\n",
        "num_units_2 = 16\n",
        "dropout_rate = 0.1\n",
        "\n",
        "# Model definition\n",
        "input_layer = Input(shape=(MAX_SEQUENCE_LENGTH,))\n",
        "embedding_layer = Embedding(input_dim=MAX_NB_WORDS, output_dim=EMBEDDING_DIM, input_length=MAX_SEQUENCE_LENGTH)(input_layer)\n",
        "\n",
        "# First GRU layer\n",
        "gru_layer1 = Bidirectional(GRU(num_units_1, return_sequences=True))(embedding_layer)\n",
        "\n",
        "# First Self-Attention layer\n",
        "attention_output1 = SelfAttention(num_units_1)(gru_layer1)\n",
        "add_layer1 = Add()([gru_layer1, attention_output1])\n",
        "norm_layer1 = LayerNormalization(epsilon=1e-6)(add_layer1)\n",
        "dropout_layer1 = Dropout(dropout_rate)(norm_layer1)\n",
        "\n",
        "# Second GRU layer\n",
        "gru_layer2 = Bidirectional(GRU(num_units_2, return_sequences=True))(dropout_layer1)\n",
        "\n",
        "# Second Self-Attention layer\n",
        "attention_output2 = SelfAttention(num_units_2)(gru_layer2)\n",
        "add_layer2 = Add()([gru_layer2, attention_output2])\n",
        "norm_layer2 = LayerNormalization(epsilon=1e-6)(add_layer2)\n",
        "dropout_layer2 = Dropout(dropout_rate)(norm_layer2)\n",
        "\n",
        "# Global average pooling\n",
        "global_avg_pooling = GlobalAveragePooling1D()(dropout_layer2)\n",
        "\n",
        "# Dense output layer\n",
        "output_layer = Dense(5, activation='softmax')(global_avg_pooling)\n",
        "\n",
        "# Build and compile the model\n",
        "model = Model(inputs=input_layer, outputs=output_layer)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compile the model\n",
        "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "num_epochs = 20\n",
        "batch_size = 256\n",
        "## For early stopping to ensure it doesnt overfit\n",
        "early_stopping = EarlyStopping(monitor='val_loss', patience=3, restore_best_weights=True)\n",
        "history = model.fit(X_train, y_train, \n",
        "                    epochs=num_epochs, batch_size=batch_size,\n",
        "                    validation_data=(X_val, y_val), callbacks=[early_stopping])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Ambil loss dari history\n",
        "train_loss = history.history['loss']\n",
        "val_loss = history.history['val_loss']\n",
        "epochs = range(1, len(train_loss) + 1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot kurva loss\n",
        "plt.plot(epochs, train_loss, 'g', label='Training Loss')\n",
        "plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n",
        "plt.title('Training and Validation Loss')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Mengevaluasi kinerja model menggunakan data validasi\n",
        "val_loss, val_acc = model.evaluate(X_val, y_val)\n",
        "print(\"Model Validation Accuracy:\", val_acc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Evaluating the Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate the model on the test set\n",
        "test_loss, test_accuracy = model.evaluate(X_test,y_test)\n",
        "print(f'Test loss: {test_loss}')\n",
        "print(f'Test accuracy: {test_accuracy}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "kuliah",
      "language": "python",
      "name": "kuliah"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
